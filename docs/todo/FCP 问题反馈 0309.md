1. Q：因集群之间显卡硬件不同导致 mmdetection 等库在更换使用集群后需要重新编译的问题

   A：可以考虑在编译脚本增加对硬件的适配列表，由 mmdetection 提供方解决。

2. Q：pytorch的分布式训练会卡在此处非常长的时间，有没有什么可以优化的地方

	![pastedImage](/Users/liuhuiya/Library/Containers/com.tencent.WeWorkMac/Data/Library/Application Support/WXWork/Data/1688851823230118/Cache/Image/2020-03/pastedImage.png)

   A：已复现，通过打印发现

![image-20200310164542303](/Users/liuhuiya/Library/Application Support/typora-user-images/image-20200310164542303.png)

读取 home 目录慢，非 pytorch 原因。

模型在使用时候会出现问题，计算数据变成 nan

![WeChatWorkScreenshot_3d6360f2-8f3f-434c-9fb8-1c1e66a0a769](/Users/liuhuiya/Library/Containers/com.tencent.WeWorkMac/Data/Library/Application Support/WXWork/Temp/ScreenCapture/WeChatWorkScreenshot_3d6360f2-8f3f-434c-9fb8-1c1e66a0a769.png)





1、现在使用pytorch都是自己装好cuda和cudnn（因为没有管理员权限，只能装在自己目录下很麻烦），然后装pytorch；这样有个问题就是有时跑不同的代码repo时候可能pytorch版本过高或者过低,又要装一遍。。。我觉得能不能在服务器上先装好一些基本库，然后使用工具管理（比如module），这样用户可以按需选择。

目前的 pytorch 镜像已经包含了 cuda 和 cudnn，检查方法：

cat /usr/local/cuda/version.txt

cat /usr/include/x86_64-linux-gnu/cudnn_v7.h | grep CUDNN_MAJOR -A 2

![image-20200309101418156](/Users/liuhuiya/Library/Application Support/typora-user-images/image-20200309101418156.png)

所以使用 release 的镜像是不需要自己安装cuda 和 cudnn的，这个会不是是使用上的问题？

目前镜像里常用的基本库都包含的，如果缺少什么常用库可以和我们说，我们会及时反馈。

关于 pytorch 版本的问题，我们的 release 镜像是跟随官方主版本的，这样能最大程度上保证框架的稳定性和效率，所以目前暂不支持多个 pytorch 版本共存。

2、缺少官方统一的文档和使用说明。现在只能在confluence上搜到一些零星的前人摸索经验，然后自己用的时候会遇到各种莫名其妙的非程序bug，而且还不知道该问谁（因为很多时候定位不到问题的原因）。影响效率

文档我们正在编写，会在 3 月底前出一版全流程的使用手册，这期间我们会完成一些部分，大家一起看下有什么需要改善的。

3.pytorch的分布式训练会卡在此处非常长的时间，有没有什么可以优化的地方

![pastedImage](/Users/liuhuiya/Library/Containers/com.tencent.WeWorkMac/Data/Library/Application Support/WXWork/Data/1688851823230118/Cache/Image/2020-03/pastedImage.png)

在多机训练时，我们在测试 pytorch 时发现默认的分布式和 apex 有概率报出 TCP 连接超时导致初始化失败。

目前该问题已解决，可以参考[https://gitlab.momenta.works/TrainingPlatform/pytorch-dev/torchland/tree/dev/demo#%E8%B8%A9%E5%9D%91%E7%82%B9](https://gitlab.momenta.works/TrainingPlatform/pytorch-dev/torchland/tree/dev/demo#踩坑点)这里面的内容，如果不是这个原因的话，我们再一起看看。

4、fcdl当读取多个源的数据(多个.pbf），使用MultiProtobufGenerator代替ProtobufGenerator时，训练会遇到Horovod报错信息（一般发生在整个训练过程的中间阶段）
RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.

关于 RuntimeError 报错的原因比较多，当模型代码出错时也会报错。所以这里能不能把全部的报错信息都发一下，我们也好看看是具体哪里的问题。

5、关于平台组的pyt问题，可不可以在各个集群上(matrix, aliyun, nv2080ti等等)都有个example，这个example下能够有：单机/多机环境部署的脚本，启动单机/多机训练测试的示例代码，要是有封装好的docker当然更好了

在 pytorch 镜像中，我们已经提供了 demo，请参考https://gitlab.momenta.works/TrainingPlatform/pytorch-dev/torchland/tree/master/demo，在 readme 有说明，如果哪里写的不清楚，随时联系我们~

6、对pyt有优化的话，每次发布新的pyt，可以跑一个fcdl下最简单的case嘛，比如就跑fcos，对比下速度与性能差异，研发人员心里有个数。

这个没问题，不过需要你们提供一下希望测试的源代码，再就是要修改发布流程中的测试环节，所以最好发 mts-request@momenta.ai 说明一下，这样我们后续都会提供这个性能数据了。